{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2992fd8-070b-4a1e-8490-4aa3842cf0f7",
   "metadata": {},
   "source": [
    "\n",
    "## DMV\n",
    "\n",
    "\n",
    "## Q-7 Analyzing Weather Data from OpenWeatherMap API\n",
    "Tasks to Perform:\n",
    "1. Register and obtain API key from OpenWeatherMap.\n",
    "2. Interact with the OpenWeatherMap API using the API key to retrieve weather data fora specific location.\n",
    "3. Extract relevant weather attributes such as temperature, humidity, wind speed, andprecipitation from the\n",
    "API response.\n",
    "4. Clean and preprocess the retrieved data, handling missing values or inconsistentformats.\n",
    "5. Perform data modeling to analyze weather patterns, such as calculating average temperature,\n",
    "maximum/minimum values, or trends over time.\n",
    "6. Visualize the weather data using appropriate plots, such as line charts, bar plots, orscatter plots, to\n",
    "represent temperature changes, precipitation levels, or wind speed variations.\n",
    "7. Apply data aggregation techniques to summarize weather statistics by specific timeperiods (e.g., daily,\n",
    "monthly, seasonal).\n",
    "8. Incorporate geographical information, if available, to create maps or geospatialvisualizations representing\n",
    "weather patterns across different locations.\n",
    "9. Explore and visualize relationships between weather attributes, such as temperatureand humidity, using\n",
    "correlation plots or heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab43444-f3cc-452a-9410-2bf727675ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "api_key = 'f06082d2638893ea5ba2bc425b43644f'\n",
    "lat = 18.184135\n",
    "lon = 74.610764   #Pune Location Co-ordinate\n",
    "# Construct the API URL for the forecast\n",
    "api_url=f\"https://api.openweathermap.org/data/2.5/forecast?lat={lat}&lon={lon}&appid={api_key}\"\n",
    "# Send a GET request to the API\n",
    "response = requests.get(api_url)\n",
    "weather_data = response.json()\n",
    "\n",
    "# Create a pandas DataFrame with the extracted weather data\n",
    "weather_df = pd.DataFrame({\n",
    "    'Timestamp': timestamps,\n",
    "    'Temperature': temperatures,\n",
    "    'Humidity': humidity,\n",
    "    'Wind Speed': wind_speed,\n",
    "    'Weather Description': weather_description,\n",
    "})\n",
    "\n",
    "weather_df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "weather_df.isnull().sum()\n",
    "\n",
    "weather_df.fillna(0, inplace=True)\n",
    "\n",
    "weather_df['Temperature'] = weather_df['Temperature'].apply(lambda x: x - 273.15) \n",
    "print(weather_df)\n",
    "\n",
    "# Plotting\n",
    "# Daily mean calculations\n",
    "daily_mean_temp = weather_df['Temperature'].resample('D').mean()\n",
    "daily_mean_humidity = weather_df['Humidity'].resample('D').mean()\n",
    "daily_mean_wind_speed = weather_df['Wind Speed'].resample('D').mean()\n",
    "\n",
    "# Plot the mean daily temperature (Line plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "daily_mean_temp.plot(color='red', marker='o')\n",
    "plt.title('Mean Daily Temperature')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.show()\n",
    "\n",
    "# Plot the mean daily humidity (Bar plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "daily_mean_humidity.plot(kind='bar', color='blue')\n",
    "plt.title('Mean Daily Humidity')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Humidity (%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of Temperature vs. Humidity\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(weather_df['Temperature'], weather_df['Humidity'])\n",
    "plt.xlabel('Temperature (°C)')\n",
    "plt.ylabel('Humidity (%)')\n",
    "plt.title('Temperature vs. Humidity Scatter Plot')\n",
    "plt.show()\n",
    "\n",
    "# Plot temperature vs. wind speed (Scatter plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(weather_df['Temperature'], weather_df['Wind Speed'], color='green')\n",
    "plt.title('Temperature vs. Wind Speed')\n",
    "plt.xlabel('Temperature (°C)')\n",
    "plt.ylabel('Wind Speed (m/s)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Heatmap of Temperature vs. Humidity\n",
    "heatmap_data = weather_df[['Temperature', 'Humidity']]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(heatmap_data, annot=True, cmap='coolwarm')\n",
    "plt.title('Temperature vs. Humidity Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d14af0-64ca-401b-97d8-535528cec3df",
   "metadata": {},
   "source": [
    "## Q-8 Analyzing Customer Churn in a Telecommunications Company\n",
    "Tasks to Perform:\n",
    "1. Import the \"Telecom_Customer_Churn.csv\" dataset.\n",
    "2. Explore the dataset to understand its structure and content.\n",
    "3. Handle missing values in the dataset, deciding on an appropriate strategy.\n",
    "4. Remove any duplicate records from the dataset.\n",
    "5. Check for inconsistent data, such as inconsistent formatting or spelling variations,andsstandardize it.\n",
    "6. Convert columns to the correct data types as needed.\n",
    "7. Identify and handle outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312c428-a690-4550-9a80-1ce6f1891fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"telecom_customer_churn.csv\")\n",
    "data.head()\n",
    "\n",
    "data.describe()\n",
    "data.info()\n",
    "data.shape\n",
    "data.nunique()\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "# Fill categorical columns with mode or specific labels\n",
    "data['Offer'].fillna(data['Offer'].mode()[0], inplace=True)\n",
    "data['Multiple Lines'].fillna(data['Multiple Lines'].mode()[0], inplace=True)\n",
    "data['Internet Type'].fillna(data['Internet Type'].mode()[0], inplace=True)\n",
    "data['Unlimited Data'].fillna(data['Unlimited Data'].mode()[0], inplace=True)\n",
    "\n",
    "data['Online Security'].fillna('nan', inplace=True)\n",
    "data['Online Backup'].fillna('nan', inplace=True)\n",
    "data['Device Protection Plan'].fillna('nan', inplace=True)\n",
    "data['Premium Tech Support'].fillna('nan', inplace=True)\n",
    "data['Streaming TV'].fillna('nan', inplace=True)\n",
    "data['Streaming Movies'].fillna('nan', inplace=True)\n",
    "data['Streaming Music'].fillna('nan', inplace=True)\n",
    "data['Churn Category'].fillna('nan', inplace=True)\n",
    "data['Churn Reason'].fillna('nan', inplace=True)\n",
    "\n",
    "# Fill numeric columns with mean or median\n",
    "data['Avg Monthly Long Distance Charges'].fillna(data['Avg Monthly Long Distance Charges'].mean(), inplace=True)\n",
    "data['Avg Monthly GB Download'].fillna(data['Avg Monthly GB Download'].mean(), inplace=True)\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "# Remove Duplicate Records\n",
    "print(\"Number of rows before removing duplicates:\", len(data))\n",
    "data_cleaned = data.drop_duplicates()\n",
    "print(\"Number of rows after removing duplicates:\", len(data_cleaned))\n",
    "\n",
    "# Measure frequency distribution for 'tenure', 'MonthlyCharges', and 'TotalCharges'\n",
    "unique_tenure, counts_tenure = np.unique(data['Tenure in Months'], return_counts=True)\n",
    "print(unique_tenure, counts_tenure)\n",
    "\n",
    "plt.scatter(unique_tenure, counts_tenure)\n",
    "plt.title('tenure vs Count')\n",
    "plt.xlabel('tenure')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "unique_monthly_charges, counts_monthly_charges = np.unique(data['Monthly Charge'], return_counts=True)\n",
    "print(unique_monthly_charges, counts_monthly_charges)\n",
    "\n",
    "plt.scatter(unique_monthly_charges, counts_monthly_charges)\n",
    "plt.title('Monthly Charges vs Count')\n",
    "plt.xlabel('Monthly Charges')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "unique_total_charges, counts_total_charges = np.unique(data['Total Charges'], return_counts=True)\n",
    "print(unique_total_charges, counts_total_charges)\n",
    "\n",
    "plt.scatter(unique_total_charges, counts_total_charges)\n",
    "plt.title('Total Charges vs Count')\n",
    "plt.xlabel('Total Charges')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize pairwise relationships\n",
    "sns.pairplot(data)\n",
    "plt.show()\n",
    "\n",
    "# Check for outliers using boxplots\n",
    "plt.boxplot(data['Tenure in Months'])\n",
    "plt.title('Boxplot of Tenure')\n",
    "\n",
    "plt.boxplot(data['Monthly Charge'])\n",
    "plt.title('Boxplot of Monthly Charges')\n",
    "plt.show()\n",
    "\n",
    "plt.boxplot(data['Total Charges'])\n",
    "plt.title('Boxplot of Monthly Charges')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Remove Outlier If Have\n",
    "#1)Z-score\n",
    "from scipy import stats\n",
    "data = data[(np.abs(stats.zscore(data)) < 3).all(axis=1)]\n",
    "\n",
    "#2)IQR\n",
    "Q1 = data['column'].quantile(0.25)\n",
    "Q3 = data['column'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "data = data[(data['column'] >= lower_bound) & (data['column'] <= upper_bound)]\n",
    "\n",
    "\n",
    "# Split the data into features and target variable\n",
    "X = data.drop(\"Churn Category\", axis=1)  # Features\n",
    "y = data[\"Churn Category\"]  # Target variable \n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Export the cleaned data\n",
    "data_cleaned.to_csv(\"Cleaned_Telecom_Customer_Churn.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1fb1a-5472-450a-887c-1ddfe6fbc5eb",
   "metadata": {},
   "source": [
    "## Q-9 Data Wrangling on Real Estate Market\n",
    "Tasks to Perform:\n",
    "1. Import the \"RealEstate_Prices.csv\" dataset. Clean column names by removing spaces, special\n",
    "characters, or renaming them for clarity.\n",
    "2. Handle missing valuesin the dataset, deciding on an appropriate strategy (e.g.,imputation or removal).\n",
    "3. Perform data merging if additional datasets with relevant information are available (e.g.,\n",
    "neighborhood demographics or nearby amenities).\n",
    "4. Filter and subset the data based on specific criteria, such as a particular time period,property type, or\n",
    "location.\n",
    "5. Handle categorical variables by encoding them appropriately (e.g., one-hot encoding or label\n",
    "encoding) for further analysis.\n",
    "6. Aggregate the data to calculate summary statistics or derived metrics such as averagesale prices by\n",
    "neighborhood or property type.\n",
    "7. Identify and handle outliers or extreme values in the data that may affect the analysisor modeling\n",
    "process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6f613-9c8b-4b70-8026-033f078e0691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"RealEstate_Prices.csv\")\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "df.describe()\n",
    "df.shape\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "df=df.dropna(subset=['location'])\n",
    "df['size'].fillna(df['size'].mode()[0], inplace=True)\n",
    "df['society'].fillna('NAN', inplace=True)\n",
    "df['bath'].fillna(df['bath'].median(), inplace=True)\n",
    "df['balcony'].fillna(df['balcony'].median(), inplace=True)\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "data=df.drop(['area_type', 'society', 'balcony', 'availability'], axis='columns')\n",
    "data\n",
    "\n",
    "# Extraxt No of Bedroom From size to New BHK Coloum\n",
    "data['bhk'] = data['size'].apply(lambda x: int(x.split(' ')[0])) \n",
    "\n",
    "# To see unique BHK values \n",
    "data.bhk.unique() \n",
    "\n",
    "## Check for unusual BHK values\n",
    "data[data.bhk > 20]\n",
    "\n",
    "def is_float(x):\n",
    "    try:\n",
    "        float(x)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# Show the first 10 rows where 'total_sqft' is not a float\n",
    "non_float_sqft_data = data[~data['total_sqft'].apply(is_float)]\n",
    "non_float_sqft_data.head(10)\n",
    "\n",
    "\n",
    "def convert_sqft_to_num(x):\n",
    "    tokens = x.split('-')\n",
    "    if len(tokens) == 2:\n",
    "        return (float(tokens[0]) + float(tokens[1])) / 2\n",
    "    try:\n",
    "        return float(x)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "data1 = data.copy()\n",
    "data1['total_sqft'] = data['total_sqft'].apply(convert_sqft_to_num)\n",
    "\n",
    "data2 = data1.copy()\n",
    "data2['price_per_sqft'] = data2['price'] * 100000 / data2['total_sqft']\n",
    "data2_stats = data2['price_per_sqft'].describe()\n",
    "\n",
    "location_stats = data2['location'].value_counts(ascending=False)\n",
    "data2.location = data2.location.apply(lambda x: x.strip())\n",
    "data2.location = data2.location.apply(lambda x: 'other' if x in location_stats[location_stats <= 10].index else x)\n",
    "\n",
    "# Remove entries with unrealistic sqft per BHK\n",
    "data2 = data2[data2.total_sqft.notnull()]\n",
    "data2 = data2[data2.total_sqft / data2.bhk >= 300]  \n",
    "\n",
    "# Remove properties where the total square feet per bedroom is less than 300\n",
    "data2 = data2[data2.total_sqft / data2.bhk >= 300]\n",
    "# Drop the 'size' column as it is no longer needed\n",
    "data2 = data2.drop('size', axis='columns')\n",
    "# One Hot Encoding for categorical variables (location)\n",
    "dummies = pd.get_dummies(data2['location'], drop_first=True)\n",
    "data2 = pd.concat([data2, dummies], axis='columns')\n",
    "# Drop the 'location' column as it's now been encoded\n",
    "data2 = data2.drop('location', axis='columns')\n",
    "# Final dataset shape\n",
    "print(data2.shape)\n",
    "# Save the cleaned data for modeling\n",
    "data2.to_csv(\"cleaned_bhp.csv\", index=False)\n",
    "# Display the head of the cleaned dataset\n",
    "data2.head()\n",
    "\n",
    "plt.boxplot(data2['price'])\n",
    "\n",
    "Q1 = data2['price'].quantile(0.25)\n",
    "Q3 = data2['price'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "data2 = data2[(data2['price'] >= lower_bound) & (data2['price'] <= upper_bound)]\n",
    "\n",
    "plt.boxplot(data2['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d8f8eb-b846-4679-a458-5a2f141bf6f2",
   "metadata": {},
   "source": [
    "## Q-10 Analyzing Air Quality Index (AQI) Trends in a City\n",
    "Tasks to Perform:\n",
    "1. Import the \"City_Air_Quality.csv\" dataset.\n",
    "2. Explore the dataset to understand its structure and content.\n",
    "3. Identify the relevant variables for visualizing AQI trends, such as date, pollutantlevels, and AQI\n",
    "values.\n",
    "4. Create line plots or time series plots to visualize the overall AQI trend over time.\n",
    "5. Plot individual pollutant levels (e.g., PM2.5, PM10, CO) on separate line plots tovisualize their trends\n",
    "over time.\n",
    "6. Use bar plots or stacked bar plots to compare the AQI values across different dates ortime periods.\n",
    "7. Create box plots or violin plots to analyze the distribution of AQI values for differentpollutant\n",
    "categories.\n",
    "8. Use scatter plots or bubble charts to explore the relationship between AQI values andpollutant levels.\n",
    "9. Customize the visualizations by adding labels, titles, legends, and appropriate colorschemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bb2ac6-1acd-4046-b6e2-aef4aa98e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv(\"air_quelity_index.csv\", encoding=\"cp1252\")\n",
    "data.head()\n",
    "\n",
    "data.info()\n",
    "\n",
    "# defining columns of importance, which shall be used reguarly\n",
    "IMP_COLS= ['so2', 'no2', 'rspm', 'spm', 'pm2_5']\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "data['so2'].fillna(data['so2'].mean(), inplace=True)\n",
    "data['no2'].fillna(data['no2'].mean(), inplace=True)\n",
    "data['rspm'].fillna(data['rspm'].mean(), inplace=True)\n",
    "data['spm'].fillna(data['spm'].mean(), inplace=True)\n",
    "data['pm2_5'].fillna(data['pm2_5'].mean(), inplace=True)\n",
    "data['type'].fillna(data['type'].mode()[0],inplace=True)\n",
    "\n",
    "data.isnull().sum()\n",
    "\n",
    "#Changing types to uniform format(Not Needed, Skip if want)\n",
    "types = {\n",
    "    \"Residential\": \"R\",\n",
    "    \"Residential and others\": \"RO\",\n",
    "    \"Residential, Rural and other Areas\": \"RRO\",\n",
    "    \"Industrial Area\": \"I\",\n",
    "    \"Industrial Areas\": \"I\",\n",
    "    \"Industrial\": \"I\",\n",
    "    \"Sensitive Area\": \"S\",\n",
    "    \"Sensitive Areas\": \"S\",\n",
    "    \"Sensitive\": \"S\",\n",
    "    np.nan: \"RRO\"\n",
    "}\n",
    "\n",
    "data.type = data.type.replace(types)\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "# Plotting highest and lowest ranking states\n",
    "# defining a function to find and plot the top 10 and bottom 10 states for a given indicator (defaults to SO2)\n",
    "def top_and_bottom_10_states(indicator=\"so2\"):\n",
    "    fig, ax = plt.subplots(2,1, figsize=(20, 12))\n",
    "    ind = data[[indicator, 'state']].groupby('state', as_index=False).median().sort_values(by=indicator,ascending=False)\n",
    "    top10 = sns.barplot(x='state', y=indicator, data=ind[:10], ax=ax[0], color='red')\n",
    "    top10.set_title(\"Top 10 states by {} (1991-2016)\".format(indicator))\n",
    "    top10.set_ylabel(\"so2 (µg/m3)\")\n",
    "    top10.set_xlabel(\"State\")\n",
    "    bottom10 = sns.barplot(x='state', y=indicator, data=ind[-10:], ax=ax[1], color='green')\n",
    "    bottom10.set_title(\"Bottom 10 states by {} (1991-2016)\".format(indicator))\n",
    "    bottom10.set_ylabel(\"so2 (µg/m3)\")\n",
    "    bottom10.set_xlabel(\"State\")\n",
    "\n",
    "top_and_bottom_10_states(\"so2\")\n",
    "top_and_bottom_10_states(\"no2\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Defining a function to find the highest ever recorded levels for a given indicator (defaults to SO2) by state\n",
    "def highest_levels_recorded(indicator=\"so2\"):\n",
    "    plt.figure(figsize=(20, 5))\n",
    "\n",
    "    # Select the maximum value for the given indicator grouped by state\n",
    "    ind = data[[indicator, 'state']].groupby('state', as_index=False).max()\n",
    "\n",
    "    # Set a different color for each state\n",
    "    num_states = ind['state'].nunique()\n",
    "    colors = sns.color_palette('husl', num_states)  # Using 'husl' palette for variety in colors\n",
    "\n",
    "    # Plot the barplot with custom colors\n",
    "    highest = sns.barplot(x='state', y=indicator, data=ind, palette=colors)\n",
    "\n",
    "    # Set title and rotate x-axis labels\n",
    "    highest.set_title(f\"Highest ever {indicator.upper()} levels recorded by state\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "highest_levels_recorded(\"no2\")\n",
    "highest_levels_recorded(\"rspm\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Plotting pollutant average by type\n",
    "# defining a function to plot pollutant averages by type for a given indicator\n",
    "def type_avg(indicator=\"\"):\n",
    "    # Selecting only numeric columns (IMP_COLS) and grouping by 'type'\n",
    "    # Excluding 'date' from the grouping as it's likely causing the TypeError\n",
    "    type_avg = data[IMP_COLS + ['type']].groupby(\"type\").mean()\n",
    "\n",
    "    if not indicator:\n",
    "        t = type_avg[indicator].plot(kind='bar')\n",
    "        plt.xticks(rotation = 0)\n",
    "        plt.title(\"Pollutant average by type for {}\".format(indicator))\n",
    "    else:\n",
    "        t = type_avg.plot(kind='bar')\n",
    "        plt.xticks(rotation = 0)\n",
    "        plt.title(\"Pollutant average by type\")\n",
    "\n",
    "type_avg('so2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def location_avgs(state, indicator=\"so2\"):\n",
    "    # Filter the data for the given state\n",
    "    state_data = data[data['state'] == state]\n",
    "\n",
    "    # Calculate the mean for each location in the state\n",
    "    locs = state_data.groupby(['location'])[IMP_COLS].mean().reset_index()\n",
    "\n",
    "    # Set a different color for each location\n",
    "    num_locations = locs['location'].nunique()\n",
    "    colors = sns.color_palette('Set2', num_locations)  # 'Set2' is just an example, you can choose different palettes\n",
    "\n",
    "    # Plot the averages using seaborn\n",
    "    sns.barplot(x='location', y=indicator, data=locs, palette=colors)\n",
    "\n",
    "    # Adding title and rotating x-axis labels\n",
    "    plt.title(f\"Location-wise average for {indicator.upper()} in {state}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "location_avgs(\"Bihar\", \"no2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b5841-4d4c-43ec-9e95-6ee367dee4fd",
   "metadata": {},
   "source": [
    "## Q-11 Analyzing Sales Performance by Region in a Retail Company\n",
    "Tasks to Perform:\n",
    "1. Import the \"Retail_Sales_Data.csv\" dataset.\n",
    "2. Explore the dataset to understand its structure and content.\n",
    "3. Identify the relevant variables for aggregating sales data, such as region, salesamount, and product\n",
    "category.\n",
    "4. Group the sales data by region and calculate the total sales amount for each region.\n",
    "5. Create bar plots or pie charts to visualize the sales distribution by region.\n",
    "6. Identify the top-performing regions based on the highest sales amount.\n",
    "7. Group the sales data by region and product category to calculate the total salesamount for each\n",
    "combination.\n",
    "8. Create stacked bar plots or grouped bar plots to compare the sales amounts acrossdifferent regions\n",
    "and product categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4018bc84-91fc-4229-89b8-ba8ea6e36726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"customer_shopping_sales_data.csv\")\n",
    "df.head()\n",
    "\n",
    "df.info()\n",
    "df.describe()\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "# To check the count of records grouped by region/branch of the mall\n",
    "df.groupby(\"shopping_mall\").count()\n",
    "\n",
    "\n",
    "# To check the count of records grouped by the product categories\n",
    "df.groupby(\"category\").count()\n",
    "\n",
    "\n",
    "# total sales for each mall branch\n",
    "branch_sales = df.groupby(\"shopping_mall\").sum()\n",
    "branch_sales\n",
    "\n",
    "\n",
    "# total sales for each category of product\n",
    "category_sales = df.groupby(\"category\").sum()\n",
    "category_sales\n",
    "\n",
    "\n",
    "# to get the top performing branches\n",
    "branch_sales.sort_values(by = \"price\", ascending = False)\n",
    "\n",
    "# to get the top selling categories\n",
    "category_sales.sort_values(by = \"price\", ascending = False)\n",
    "\n",
    "# to get total sales for each combination of branch and product_category\n",
    "combined_branch_category_sales = df.groupby([\"shopping_mall\", \"category\"]).sum()\n",
    "combined_branch_category_sales\n",
    "\n",
    "\n",
    "# pie chart for sales by branch\n",
    "plt.pie(branch_sales[\"price\"], labels = branch_sales.index)\n",
    "plt.show()\n",
    "\n",
    "# pie chart for sales by product category\n",
    "plt.pie(category_sales[\"price\"], labels = category_sales.index)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "combined_pivot = df.pivot_table(index=\"shopping_mall\", columns=\"category\", values=\"price\", aggfunc=\"sum\")\n",
    "# grouped bar chart for sales of different categories at different branches\n",
    "combined_pivot.plot(kind=\"bar\", figsize=(10, 6))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
